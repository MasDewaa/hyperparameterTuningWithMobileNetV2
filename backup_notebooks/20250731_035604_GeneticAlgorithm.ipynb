{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Hyperparameter Tuning for Batik Classification\n",
    "\n",
    "This notebook implements a custom Genetic Algorithm for hyperparameter tuning for the batik classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Third-party packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import TFSMLayer\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    GlobalAveragePooling2D,\n",
    "    Dense,\n",
    "    Dropout\n",
    ")\n",
    "from tensorflow.keras.applications import InceptionV3, MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    "    load_img,\n",
    "    img_to_array\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    Callback\n",
    ")\n",
    "\n",
    "# Hilangkan warning TensorFlow dan Python\n",
    "import warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow mendeteksi GPU (CUDA):\n",
      "  - /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"✅ TensorFlow mendeteksi GPU (CUDA):\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "else:\n",
    "    print(\"⚠️ TensorFlow TIDAK mendeteksi GPU. Model berjalan di CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gambar : 1080\n",
      "\n",
      "Sebaran Data per Kelas:\n",
      "                  Train  Validation  Test\n",
      "Sekar Pijetan        12           3     3\n",
      "Sekar Pacar          12           3     3\n",
      "Gedhangan            12           3     3\n",
      "Sekar Keben          12           3     3\n",
      "Sekar Jali           12           3     3\n",
      "Mawur                12           3     3\n",
      "Sekar Duren          12           3     3\n",
      "Sekar Dlima          12           3     3\n",
      "Jayakirana           12           3     3\n",
      "Cinde Wilis          12           3     3\n",
      "Sekar Blimbing       12           3     3\n",
      "Sekar Ketongkeng     12           3     3\n",
      "Sekar Kemuning       12           3     3\n",
      "Sekar Srengenge      12           3     3\n",
      "Sekar Tebu           12           3     3\n",
      "Sekar Kepel          12           3     3\n",
      "Sekar Lintang        12           3     3\n",
      "Sekar Kenthang       12           3     3\n",
      "Sekar Pala           12           3     3\n",
      "Sekar Mlathi         12           3     3\n",
      "Sekar Randhu         12           3     3\n",
      "Jayakusuma           12           3     3\n",
      "Sekar Manggis        12           3     3\n",
      "Krawitan             12           3     3\n",
      "Sritaman             12           3     3\n",
      "Sekar Mrica          12           3     3\n",
      "Rengganis            12           3     3\n",
      "Sekar Jagung         12           3     3\n",
      "Sekar Nangka         12           3     3\n",
      "Sekar Tanjung        12           3     3\n",
      "Sekar Liring         12           3     3\n",
      "Cakar Ayam           12           3     3\n",
      "Sekar Andhong        12           3     3\n",
      "Klampok Arum         12           3     3\n",
      "Kemukus              12           3     3\n",
      "Sekar Kenikir        12           3     3\n",
      "Sekar Sawo           12           3     3\n",
      "Sekar Kenanga        12           3     3\n",
      "Tanjung Gunung       12           3     3\n",
      "Sekar Gudhe          12           3     3\n",
      "Worawari Rumpuk      12           3     3\n",
      "Sekar Gambir         12           3     3\n",
      "Manggar              12           3     3\n",
      "Sekar Menur          12           3     3\n",
      "Sekar Pudhak         12           3     3\n",
      "Sekar Dangan         12           3     3\n",
      "Kuncup Kanthil       12           3     3\n",
      "Truntum Kurung       12           3     3\n",
      "Arumdalu             12           3     3\n",
      "Sekar Cengkeh        12           3     3\n",
      "Sekar Mindi          12           3     3\n",
      "Sari Mulat           12           3     3\n",
      "Brendhi              12           3     3\n",
      "Sekar Soka           12           3     3\n",
      "Sekar Gayam          12           3     3\n",
      "Sekar Jeruk          12           3     3\n",
      "Sekar Mundhu         12           3     3\n",
      "Kawung Nitik         12           3     3\n",
      "Sekar Srigadhing     12           3     3\n",
      "Sekar Dhuku          12           3     3\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join('../../../../data/splits/dataset_split', 'train')\n",
    "val_dir = os.path.join('../../../../data/splits/dataset_split', 'val')\n",
    "test_dir = os.path.join('../../../../data/splits/dataset_split', 'test')\n",
    "\n",
    "# Fungsi untuk menghitung jumlah file per kelas dalam sebuah direktori\n",
    "def count_images_per_class(directory):\n",
    "    return {\n",
    "        cls: len(os.listdir(os.path.join(directory, cls)))\n",
    "        for cls in os.listdir(directory)\n",
    "        if os.path.isdir(os.path.join(directory, cls))\n",
    "    }\n",
    "    \n",
    "# Hitung jumlah gambar per kelas\n",
    "train_counts = count_images_per_class(train_dir)\n",
    "val_counts = count_images_per_class(val_dir)\n",
    "test_counts = count_images_per_class(test_dir)\n",
    "total_gambar = sum(train_counts.values()) + sum(val_counts.values()) + sum(test_counts.values())\n",
    "\n",
    "# Gabungkan ke dalam DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Train': train_counts,\n",
    "    'Validation': val_counts,\n",
    "    'Test': test_counts\n",
    "}).T.fillna(0).astype(int).T  # Transpose agar kelas sebagai indeks\n",
    "\n",
    "\n",
    "print(f\"Total gambar : {total_gambar}\")\n",
    "# Tampilkan sebagai tabel\n",
    "print(\"\\nSebaran Data per Kelas:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 4\n",
    "SEED = 42\n",
    "NUM_CLASSES = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,                # Normalisasi piksel gambar dari 0-255 menjadi 0-1\n",
    "    rotation_range=20,            # Rotasi gambar acak hingga 20 derajat\n",
    "    zoom_range=0.2,               # Zoom acak hingga 20% untuk mensimulasikan perbedaan jarak\n",
    "    width_shift_range=0.2,        # Geser gambar secara horizontal hingga 20% lebar gambar\n",
    "    height_shift_range=0.2,       # Geser gambar secara vertikal hingga 20% tinggi gambar\n",
    "    shear_range=0.15,             # Distorsi gambar secara miring (shear)\n",
    "    horizontal_flip=True,         # Membalik gambar secara horizontal (misalnya daun kiri dan kanan)\n",
    "    brightness_range=[0.8, 1.2],  # Variasi pencahayaan gambar\n",
    "    fill_mode='nearest'           # Isi area kosong hasil transformasi dengan piksel terdekat\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720 images belonging to 60 classes.\n",
      "Found 180 images belonging to 60 classes.\n",
      "Found 180 images belonging to 60 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # Folder berisi data latih\n",
    "    target_size = IMG_SIZE,         # Ukuran gambar diubah menjadi 224x224\n",
    "    batch_size = BATCH_SIZE,        # Jumlah gambar per batch\n",
    "    class_mode = 'categorical',       # Label dalam format one-hot (karena klasifikasi multi-kelas)\n",
    "    shuffle = True,                   # Acak data untuk melatih model dengan lebih baik\n",
    "    seed = SEED                         # Seed untuk konsistensi hasil saat diacak\n",
    ")\n",
    "\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,  # Folder validasi\n",
    "    target_size = IMG_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False,                  # Tidak diacak agar evaluasi konsisten\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,   # Folder pengujian\n",
    "    target_size = IMG_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False,                  # Tidak diacak agar prediksi bisa dibandingkan langsung\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Callback untuk progress log\n",
    "class TuningCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tuner_name):\n",
    "        super().__init__()\n",
    "        self.tuner_name = tuner_name\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metrics_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
    "        print(f\"✨ {self.tuner_name} - Epoch {epoch + 1} — {metrics_str}\")\n",
    "\n",
    "# =========== GENETIC ALGORITHM IMPLEMENTATION ===========\n",
    "class GeneticAlgorithm:\n",
    "    def __init__(self, population_size=10, generations=20, mutation_rate=0.1, crossover_rate=0.8):\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.best_individual = None\n",
    "        self.best_fitness = 0\n",
    "        self.history = []\n",
    "        \n",
    "    def create_individual(self) -> Dict[str, Any]:\n",
    "        \"\"\"Create a random individual (hyperparameter set)\"\"\"\n",
    "        return {\n",
    "            'learning_rate': random.choice([1e-4, 3e-4, 1e-3, 3e-3, 1e-2]),\n",
    "            'optimizer': random.choice(['adam', 'rmsprop', 'sgd']),\n",
    "            'dense_units': random.choice([64, 128, 256, 384, 512]),\n",
    "            'dropout_rate': random.choice([0.0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "            'add_conv_layer': random.choice([True, False]),\n",
    "            'conv_filters': random.choice([32, 64, 96, 128])\n",
    "        }\n",
    "    \n",
    "    def create_population(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create initial population\"\"\"\n",
    "        return [self.create_individual() for _ in range(self.population_size)]\n",
    "    \n",
    "    def build_model_from_individual(self, individual: Dict[str, Any]) -> tf.keras.Model:\n",
    "        \"\"\"Build model from individual hyperparameters\"\"\"\n",
    "        # Load pre-trained MobileNetV2 dengan freezing\n",
    "        base_model = MobileNetV2(\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "        \n",
    "        # Freeze semua layer pada base model\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        \n",
    "        # Tambahkan ConvLayer (opsional)\n",
    "        if individual['add_conv_layer']:\n",
    "            model.add(Conv2D(individual['conv_filters'], (3, 3), activation='relu', padding='same'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # GlobalAveragePooling2D untuk flatten feature map\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(Dense(individual['dense_units'], activation='relu'))\n",
    "        \n",
    "        # Dropout untuk regularisasi\n",
    "        if individual['dropout_rate'] > 0:\n",
    "            model.add(Dropout(individual['dropout_rate']))\n",
    "        \n",
    "        # Layer output\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        \n",
    "        # Kompilasi dengan hyperparameter\n",
    "        if individual['optimizer'] == 'adam':\n",
    "            optimizer = Adam(learning_rate=individual['learning_rate'])\n",
    "        elif individual['optimizer'] == 'rmsprop':\n",
    "            optimizer = RMSprop(learning_rate=individual['learning_rate'])\n",
    "        else:\n",
    "            optimizer = SGD(learning_rate=individual['learning_rate'], momentum=0.9)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def evaluate_individual(self, individual: Dict[str, Any]) -> float:\n",
    "        \"\"\"Evaluate individual by training model and returning validation accuracy\"\"\"\n",
    "        try:\n",
    "            model = self.build_model_from_individual(individual)\n",
    "            \n",
    "            # Callbacks\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                restore_best_weights=True,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                validation_data=val_generator,\n",
    "                epochs=10,  # Shorter training for faster evaluation\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Return best validation accuracy\n",
    "            best_val_acc = max(history.history['val_accuracy'])\n",
    "            return best_val_acc\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating individual: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def select_parents(self, population: List[Dict[str, Any]], fitness_scores: List[float]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "        \"\"\"Select parents using tournament selection\"\"\"\n",
    "        tournament_size = 3\n",
    "        \n",
    "        # Select first parent\n",
    "        tournament1 = random.sample(list(enumerate(population)), tournament_size)\n",
    "        parent1_idx = max(tournament1, key=lambda x: fitness_scores[x[0]])[0]\n",
    "        \n",
    "        # Select second parent\n",
    "        tournament2 = random.sample(list(enumerate(population)), tournament_size)\n",
    "        parent2_idx = max(tournament2, key=lambda x: fitness_scores[x[0]])[0]\n",
    "        \n",
    "        return population[parent1_idx], population[parent2_idx]\n",
    "    \n",
    "    def crossover(self, parent1: Dict[str, Any], parent2: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "        \"\"\"Perform crossover between two parents\"\"\"\n",
    "        if random.random() > self.crossover_rate:\n",
    "            return parent1, parent2\n",
    "        \n",
    "        child1 = copy.deepcopy(parent1)\n",
    "        child2 = copy.deepcopy(parent2)\n",
    "        \n",
    "        # Single point crossover for each parameter\n",
    "        for key in parent1.keys():\n",
    "            if random.random() < 0.5:\n",
    "                child1[key], child2[key] = child2[key], child1[key]\n",
    "        \n",
    "        return child1, child2\n",
    "    \n",
    "    def mutate(self, individual: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Mutate individual with some probability\"\"\"\n",
    "        mutated = copy.deepcopy(individual)\n",
    "        \n",
    "        for key in mutated.keys():\n",
    "            if random.random() < self.mutation_rate:\n",
    "                if key == 'learning_rate':\n",
    "                    mutated[key] = random.choice([1e-4, 3e-4, 1e-3, 3e-3, 1e-2])\n",
    "                elif key == 'optimizer':\n",
    "                    mutated[key] = random.choice(['adam', 'rmsprop', 'sgd'])\n",
    "                elif key == 'dense_units':\n",
    "                    mutated[key] = random.choice([64, 128, 256, 384, 512])\n",
    "                elif key == 'dropout_rate':\n",
    "                    mutated[key] = random.choice([0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "                elif key == 'add_conv_layer':\n",
    "                    mutated[key] = random.choice([True, False])\n",
    "                elif key == 'conv_filters':\n",
    "                    mutated[key] = random.choice([32, 64, 96, 128])\n",
    "        \n",
    "        return mutated\n",
    "    \n",
    "    def evolve(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run genetic algorithm\"\"\"\n",
    "        print(f\"🚀 Memulai Genetic Algorithm dengan {self.population_size} individu dan {self.generations} generasi\")\n",
    "        \n",
    "        # Create initial population\n",
    "        population = self.create_population()\n",
    "        \n",
    "        for generation in range(self.generations):\n",
    "            print(f\"\\n🔄 Generasi {generation + 1}/{self.generations}\")\n",
    "            \n",
    "            # Evaluate all individuals\n",
    "            fitness_scores = []\n",
    "            for i, individual in enumerate(population):\n",
    "                print(f\"  Evaluating individual {i+1}/{len(population)}...\")\n",
    "                fitness = self.evaluate_individual(individual)\n",
    "                fitness_scores.append(fitness)\n",
    "                print(f\"    Fitness: {fitness:.4f}\")\n",
    "            \n",
    "            # Update best individual\n",
    "            best_idx = np.argmax(fitness_scores)\n",
    "            if fitness_scores[best_idx] > self.best_fitness:\n",
    "                self.best_fitness = fitness_scores[best_idx]\n",
    "                self.best_individual = copy.deepcopy(population[best_idx])\n",
    "                print(f\"  🎉 New best fitness: {self.best_fitness:.4f}\")\n",
    "            \n",
    "            # Record history\n",
    "            self.history.append({\n",
    "                'generation': generation + 1,\n",
    "                'best_fitness': max(fitness_scores),\n",
    "                'avg_fitness': np.mean(fitness_scores),\n",
    "                'best_individual': copy.deepcopy(population[best_idx])\n",
    "            })\n",
    "            \n",
    "            # Create new population\n",
    "            new_population = []\n",
    "            \n",
    "            # Elitism: keep best individual\n",
    "            new_population.append(copy.deepcopy(population[best_idx]))\n",
    "            \n",
    "            # Generate rest of population through selection, crossover, and mutation\n",
    "            while len(new_population) < self.population_size:\n",
    "                parent1, parent2 = self.select_parents(population, fitness_scores)\n",
    "                child1, child2 = self.crossover(parent1, parent2)\n",
    "                \n",
    "                child1 = self.mutate(child1)\n",
    "                child2 = self.mutate(child2)\n",
    "                \n",
    "                new_population.extend([child1, child2])\n",
    "            \n",
    "            # Trim to population size\n",
    "            population = new_population[:self.population_size]\n",
    "            \n",
    "            print(f\"  Best fitness this generation: {max(fitness_scores):.4f}\")\n",
    "            print(f\"  Average fitness: {np.mean(fitness_scores):.4f}\")\n",
    "        \n",
    "        print(f\"\\n🏆 Genetic Algorithm selesai!\")\n",
    "        print(f\"Best fitness achieved: {self.best_fitness:.4f}\")\n",
    "        print(f\"Best individual: {self.best_individual}\")\n",
    "        \n",
    "        return self.best_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Memulai Genetic Algorithm dengan 8 individu dan 10 generasi\n",
      "\n",
      "🔄 Generasi 1/10\n",
      "  Evaluating individual 1/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753813141.620624   69488 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753813146.423984   69621 service.cc:152] XLA service 0x7f65ac002260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753813146.424065   69621 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2025-07-30 01:19:06.505958: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1753813147.172625   69621 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1753813154.270657   69621 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-07-30 01:19:21.744363: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1218', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.9778\n",
      "  Evaluating individual 2/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:20:25.366441: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4343', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-07-30 01:20:25.719590: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4343', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:20:25.791529: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4343', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-30 01:20:25.884202: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4343_0', 184 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-07-30 01:20:26.325658: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4343', 4456 bytes spill stores, 4516 bytes spill loads\n",
      "\n",
      "2025-07-30 01:20:26.701370: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4343', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:20:26.779961: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4343', 4220 bytes spill stores, 4204 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.9444\n",
      "  Evaluating individual 3/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:21:37.899524: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-07-30 01:21:38.098687: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-30 01:21:38.223585: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331_0', 216 bytes spill stores, 556 bytes spill loads\n",
      "\n",
      "2025-07-30 01:21:38.647597: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 4220 bytes spill stores, 4204 bytes spill loads\n",
      "\n",
      "2025-07-30 01:21:38.666797: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:21:39.148317: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 4456 bytes spill stores, 4516 bytes spill loads\n",
      "\n",
      "2025-07-30 01:21:39.264433: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-07-30 01:21:48.717105: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1202', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.6167\n",
      "  Evaluating individual 4/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:22:32.078174: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:22:32.511345: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-30 01:22:32.586991: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:22:32.730309: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 4456 bytes spill stores, 4516 bytes spill loads\n",
      "\n",
      "2025-07-30 01:22:33.004622: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 4220 bytes spill stores, 4204 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.2278\n",
      "  Evaluating individual 5/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:23:34.595508: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-07-30 01:23:34.721447: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:23:35.005310: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331_0', 184 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-07-30 01:23:35.199982: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-30 01:23:35.541477: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 4456 bytes spill stores, 4516 bytes spill loads\n",
      "\n",
      "2025-07-30 01:23:35.962951: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 4220 bytes spill stores, 4204 bytes spill loads\n",
      "\n",
      "2025-07-30 01:23:36.297768: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4331', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:23:45.807760: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1202', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.9722\n",
      "  Evaluating individual 6/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:24:45.866336: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4393', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.0667\n",
      "  Evaluating individual 7/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:25:54.214689: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4055', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:25:54.485544: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4055_0', 184 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-07-30 01:25:54.585159: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4055', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-07-30 01:25:54.638608: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4055', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-07-30 01:25:54.985004: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4055', 4220 bytes spill stores, 4204 bytes spill loads\n",
      "\n",
      "2025-07-30 01:25:55.081791: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4055', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-07-30 01:25:55.487027: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4055', 4456 bytes spill stores, 4516 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.9833\n",
      "  Evaluating individual 8/8...\n",
      "    Fitness: 0.0556\n",
      "  🎉 New best fitness: 0.9833\n",
      "  Best fitness this generation: 0.9833\n",
      "  Average fitness: 0.6056\n",
      "\n",
      "🔄 Generasi 2/10\n",
      "  Evaluating individual 1/8...\n",
      "    Fitness: 0.9833\n",
      "  Evaluating individual 2/8...\n",
      "    Fitness: 0.9722\n",
      "  Evaluating individual 3/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:30:28.858871: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4465', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fitness: 0.9833\n",
      "  Evaluating individual 4/8...\n",
      "    Fitness: 0.0222\n",
      "  Evaluating individual 5/8...\n",
      "    Fitness: 0.9778\n",
      "  Evaluating individual 6/8...\n",
      "    Fitness: 0.9556\n",
      "  Evaluating individual 7/8...\n",
      "    Fitness: 0.9833\n",
      "  Evaluating individual 8/8...\n",
      "    Fitness: 0.9778\n",
      "  Best fitness this generation: 0.9833\n",
      "  Average fitness: 0.8569\n",
      "\n",
      "🔄 Generasi 3/10\n",
      "  Evaluating individual 1/8...\n",
      "    Fitness: 0.9722\n",
      "  Evaluating individual 2/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 01:40:21.150157: E external/local_xla/xla/service/slow_operation_alarm.cc:73] \n",
      "********************************\n",
      "[Compiling module a_inference_one_step_on_data_411444__.1378] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "# =========== RUN GENETIC ALGORITHM ===========\n",
    "\n",
    "# Buat direktori untuk menyimpan hasil tuning\n",
    "project_dir = \"hyperparameter_tuning_genetic_results\"\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "# Initialize genetic algorithm\n",
    "ga = GeneticAlgorithm(\n",
    "    population_size=8,  # Smaller population for faster execution\n",
    "    generations=10,     # Fewer generations for demonstration\n",
    "    mutation_rate=0.1,\n",
    "    crossover_rate=0.8\n",
    ")\n",
    "\n",
    "# Run genetic algorithm\n",
    "best_hyperparameters = ga.evolve()\n",
    "\n",
    "print(\"\\n==== HYPERPARAMETER TERBAIK ====\")\n",
    "for key, value in best_hyperparameters.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train final model with best hyperparameters\n",
    "print(\"\\n==== LATIH MODEL FINAL DENGAN HYPERPARAMETER TERBAIK ====\")\n",
    "\n",
    "# Build model with best hyperparameters\n",
    "best_model = ga.build_model_from_individual(best_hyperparameters)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Callbacks for final training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "history = best_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        ModelCheckpoint('best_genetic_model.keras', save_best_only=True, monitor='val_accuracy'),\n",
    "        reduce_lr\n",
    "    ],\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save best model\n",
    "best_model.save('final_genetic_model.keras')\n",
    "print(\"Model final telah disimpan sebagai 'final_genetic_model.keras'\")\n",
    "\n",
    "# Evaluate model on test set\n",
    "print(\"\\n==== EVALUASI MODEL PADA TEST SET ====\")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = best_model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize genetic algorithm progress\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot fitness evolution\n",
    "plt.subplot(1, 3, 1)\n",
    "generations = [h['generation'] for h in ga.history]\n",
    "best_fitness = [h['best_fitness'] for h in ga.history]\n",
    "avg_fitness = [h['avg_fitness'] for h in ga.history]\n",
    "\n",
    "plt.plot(generations, best_fitness, 'b-', label='Best Fitness', linewidth=2)\n",
    "plt.plot(generations, avg_fitness, 'r--', label='Average Fitness', linewidth=2)\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Fitness (Validation Accuracy)')\n",
    "plt.title('Genetic Algorithm Evolution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('genetic_algorithm_results.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Genetic algorithm results telah disimpan sebagai 'genetic_algorithm_results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of genetic algorithm results\n",
    "print(\"\\n==== ANALISIS HASIL GENETIC ALGORITHM ====\")\n",
    "print(f\"Total generations: {len(ga.history)}\")\n",
    "print(f\"Population size: {ga.population_size}\")\n",
    "print(f\"Mutation rate: {ga.mutation_rate}\")\n",
    "print(f\"Crossover rate: {ga.crossover_rate}\")\n",
    "print(f\"Best fitness achieved: {ga.best_fitness:.4f}\")\n",
    "\n",
    "print(\"\\nEvolution of best fitness:\")\n",
    "for i, record in enumerate(ga.history):\n",
    "    print(f\"Generation {record['generation']}: {record['best_fitness']:.4f}\")\n",
    "\n",
    "print(\"\\nComparison with other methods:\")\n",
    "print(\"Genetic Algorithm advantages:\")\n",
    "print(\"- Can find good solutions with fewer evaluations\")\n",
    "print(\"- Maintains diversity in search space\")\n",
    "print(\"- Can escape local optima through mutation\")\n",
    "print(\"- More efficient than GridSearch for large search spaces\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
