# PERBANDINGAN HYPERPARAMETER TUNING BERBASIS TRANSFER LEARNING DALAM KLASIFIKASI CITRA BATIK

## ğŸ“‹ Overview

Project ini mengimplementasikan perbandingan komprehensif metode hyperparameter tuning pada model klasifikasi citra batik Indonesia menggunakan Transfer Learning MobileNetV2. Penelitian ini membandingkan performa dan efisiensi komputasional dari tiga metode tuning: **Grid Search**, **Random Search**, dan **Genetic Algorithm**.

## ğŸ¯ Research Objectives

### Rumusan Masalah
1. Bagaimana perbandingan performa accuracy, precision, recall, dan F1-score model klasifikasi citra batik berbasis Transfer Learning ketika dioptimalkan menggunakan berbagai metode Hyperparameter Tuning seperti Grid Search, Random Search, Genetic Algorithm?

2. Bagaimana perbandingan efisiensi komputasional waktu tuning, sumber daya yang dibutuhkan antara metode-metode Hyperparameter Tuning yang diterapkan pada model Transfer Learning untuk klasifikasi citra batik?

### Tujuan Penelitian
1. Menghasilkan analisis perbandingan kinerja MobileNetV2 setelah dan sebelum diterapkan Hyperparameter Tuning dengan metode Grid Search, Random Search, dan Genetic Algorithm.

2. Menghasilkan data perbandingan terukur mengenai efisiensi komputasional meliputi waktu tuning dan estimasi kebutuhan sumber daya dari penerapan ketiga metode Hyperparameter Tuning tersebut pada model Transfer Learning MobileNetV2 untuk tugas klasifikasi citra batik.

## ğŸ—ï¸ Project Structure

```
Batik-Final/
â”œâ”€â”€ data/                           # Dataset directory
â”‚   â”œâ”€â”€ train/                     # Training images (720 images)
â”‚   â”œâ”€â”€ val/                       # Validation images (180 images)
â”‚   â””â”€â”€ test/                      # Test images (180 images)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ origin/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.ipynb        # Baseline model implementation
â”‚   â”‚   â”‚   â””â”€â”€ baseline_evaluation.py
â”‚   â”‚   â””â”€â”€ hyperparameterTuning/
â”‚   â”‚       â”œâ”€â”€ gridSearch/
â”‚   â”‚       â”‚   â””â”€â”€ GridSearch.ipynb
â”‚   â”‚       â”œâ”€â”€ randomSearch/
â”‚   â”‚       â”‚   â””â”€â”€ RandomSearch.ipynb
â”‚   â”‚       â”œâ”€â”€ geneticAlgorithm/
â”‚   â”‚       â”‚   â””â”€â”€ GeneticAlgorithm_New.ipynb
â”‚   â”‚       â””â”€â”€ README.md
â”‚   â”œâ”€â”€ result/
â”‚   â”‚   â”œâ”€â”€ comparison_analysis.py
â”‚   â”‚   â”œâ”€â”€ README_COMPARISON.md
â”‚   â”‚   â””â”€â”€ [result directories]
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ api/
â”œâ”€â”€ test/
â”œâ”€â”€ backup_notebooks/
â””â”€â”€ README.md
```

## ğŸ“Š Dataset Information

### Dataset Characteristics
- **Total Images**: 1,080
- **Classes**: 60 different Indonesian batik patterns
- **Split**: 720 train / 180 validation / 180 test
- **Image Size**: 224x224 pixels
- **Format**: RGB images
- **Distribution**: 12 images per class (train), 3 images per class (val/test)

### Batik Classes (60 patterns)
Traditional Indonesian batik patterns including:
- Sekar Pijetan, Sekar Pacar, Gedhangan
- Sekar Keben, Sekar Jali, Mawur
- Sekar Duren, Sekar Dlima, Jayakirana
- Cinde Wilis, Sekar Blimbing, Sekar Ketongkeng
- And 48 more traditional batik patterns...

## ğŸ¤– Model Architecture

### Base Model: MobileNetV2
- **Architecture**: Transfer learning with MobileNetV2
- **Pre-trained Weights**: ImageNet
- **Input Shape**: (224, 224, 3)
- **Feature Extraction**: Global Average Pooling

### Custom Classification Head
```python
Sequential([
    MobileNetV2(base),           # Feature extractor
    GlobalAveragePooling2D(),    # Flatten features
    Dense(128, activation='relu'), # Classification layer
    Dropout(0.2),               # Regularization
    Dense(60, activation='softmax') # Output layer
])
```

### Hyperparameters Optimized
1. **Learning Rate**: [1e-4, 1e-3, 3e-3]
2. **Optimizer**: [Adam, RMSprop, SGD]
3. **Dense Units**: [128, 256]
4. **Dropout Rate**: [0.1, 0.2, 0.3]
5. **Add Conv Layer**: [True, False]
6. **Conv Filters**: [32, 64]

## ğŸ“ˆ Actual Results

### Model Performance Comparison

| Method | Accuracy | Precision | Recall | F1-Score | Test Loss | Training Time |
|--------|----------|-----------|--------|----------|-----------|---------------|
| **Baseline** | 95.00% | 0.950 | 0.950 | 0.950 | 0.150 | 3.35 min |
| **Genetic Algorithm** | **98.33%** | **0.983** | **0.983** | **0.983** | **0.118** | 16.96 min |
| **Grid Search** | 97.78% | 0.978 | 0.978 | 0.978 | 0.122 | 24.24 min |
| **Random Search** | 96.11% | 0.961 | 0.961 | 0.961 | 0.135 | 28.01 min |

### Computational Efficiency Ranking

| Method | Accuracy/Minute | Efficiency Score |
|--------|----------------|-----------------|
| **Baseline** | 0.284 | Highest |
| **Genetic Algorithm** | 0.058 | Best Balance |
| **Grid Search** | 0.040 | Moderate |
| **Random Search** | 0.034 | Lowest |

### Optimal Hyperparameters Found

#### Genetic Algorithm (Best Performance)
```python
{
    'learning_rate': 2.26e-4,
    'optimizer': 'adam',
    'dense_units': 128,
    'dropout_rate': 0.2,
    'add_conv_layer': False,
    'conv_filters': 32
}
```

#### Grid Search
```python
{
    'learning_rate': 1e-3,
    'optimizer': 'adam',
    'dense_units': 256,
    'dropout_rate': 0.2,
    'add_conv_layer': False,
    'conv_filters': 32
}
```

#### Random Search
```python
{
    'learning_rate': 1.54e-4,
    'optimizer': 'rmsprop',
    'dense_units': 128,
    'dropout_rate': 0.2,
    'add_conv_layer': False,
    'conv_filters': 32
}
```

## ğŸš€ Usage

### Prerequisites
```bash
# Install required packages
pip install tensorflow keras numpy pandas matplotlib seaborn
pip install pillow tqdm scikit-learn kerastuner psutil
```

### Quick Start

#### 1. Baseline Model
```bash
cd src/models/origin
jupyter notebook main.ipynb
```

#### 2. Hyperparameter Tuning

**Genetic Algorithm** (Recommended - Best Performance):
```bash
cd src/models/hyperparameterTuning/geneticAlgorithm
jupyter notebook GeneticAlgorithm_New.ipynb
```

**Grid Search** (Comprehensive Search):
```bash
cd src/models/hyperparameterTuning/gridSearch
jupyter notebook GridSearch.ipynb
```

**Random Search** (Quick Exploration):
```bash
cd src/models/hyperparameterTuning/randomSearch
jupyter notebook RandomSearch.ipynb
```

#### 3. Results Analysis
```bash
cd src/result
python comparison_analysis.py
```

### Training Process

1. **Data Loading**: Images are loaded and preprocessed
2. **Data Augmentation**: Applied to training data only
3. **Model Training**: Transfer learning with fine-tuning
4. **Evaluation**: Performance metrics on test set
5. **Model Saving**: Best model saved for deployment

## ğŸ”§ Configuration

### Data Augmentation
```python
ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2]
)
```

### Training Parameters
- **Batch Size**: 8
- **Epochs**: 30 (with early stopping)
- **Learning Rate**: Adaptive (ReduceLROnPlateau)
- **Patience**: 5 epochs
- **Class Weights**: Balanced

## ğŸ“Š Key Findings

### ğŸ† Best Performance: Genetic Algorithm
- **Accuracy**: 98.33% (highest)
- **Training Time**: 16.96 minutes (efficient)
- **Best Fitness**: 0.9333
- **Generations**: 5 with population size 3
- **Convergence**: Fast and stable

### ğŸ¯ Grid Search Analysis
- **Accuracy**: 97.78% (second best)
- **Coverage**: 100% of search space
- **Trials**: 16 combinations
- **Deterministic**: Reproducible results
- **Time**: 24.24 minutes

### âš¡ Random Search Analysis
- **Accuracy**: 96.11% (moderate)
- **Trials**: 10 random samples
- **Stochastic**: Results may vary
- **Time**: 28.01 minutes (longest)
- **Efficiency**: Lower than expected

### ğŸ“ˆ Baseline Performance
- **Accuracy**: 95.00% (good baseline)
- **Time**: 3.35 minutes (fastest)
- **Efficiency**: 0.284 accuracy/minute
- **Use Case**: Quick testing and prototyping

## ğŸ¯ Recommendations

### For Production Deployment
- **Use Genetic Algorithm**: Best performance (98.33%)
- **Configuration**: Population size 3-5, generations 5-10
- **Hyperparameters**: Learning rate 2e-4, Adam optimizer
- **Monitoring**: Implement early stopping and model checkpointing

### For Research & Development
- **Use Grid Search**: Comprehensive and deterministic
- **Configuration**: Systematic parameter combinations
- **Documentation**: Document all combinations for analysis
- **Validation**: Cross-validation for robustness

### For Quick Testing
- **Use Baseline Model**: Fastest evaluation (3.35 min)
- **Configuration**: Standard parameters
- **Use Case**: Prototyping and concept validation
- **Efficiency**: Highest accuracy per minute

### For Balanced Approach
- **Use Genetic Algorithm with Optimization**: Best balance
- **Configuration**: Population size 3, generations 5
- **Cross-validation**: Implement k-fold cross-validation
- **Monitoring**: Monitor overfitting with validation set

## ğŸ”¬ Research Contributions

### Theoretical Contributions
1. **Transfer Learning Validation**: Proves effectiveness of MobileNetV2 for Indonesian batik classification
2. **Hyperparameter Tuning Comparison**: Comprehensive comparison of three tuning methods
3. **Optimal Configuration Discovery**: Finds optimal hyperparameters for batik classification
4. **Computational Efficiency Analysis**: Trade-off analysis between performance and efficiency

### Practical Contributions
1. **Implementation Guidelines**: Practical guide for hyperparameter tuning implementation
2. **Best Practices**: Establishes best practices for batik domain
3. **Cost-Benefit Analysis**: Provides cost-benefit analysis for method selection
4. **Decision Framework**: Framework for choosing tuning method based on requirements

## ğŸ“ Technical Notes

- **GPU Recommended**: Training is significantly faster with GPU
- **Memory Requirements**: ~8GB RAM recommended
- **Storage**: ~2GB for dataset and models
- **Compatibility**: Python 3.8+, TensorFlow 2.x
- **Reproducibility**: All experiments use fixed random seeds

## ğŸš€ Future Work

### Dataset Enhancement
- **Data Augmentation**: Advanced techniques (CutMix, MixUp)
- **Dataset Expansion**: 50-100 images per class
- **Quality Improvement**: Higher resolution, better lighting
- **External Validation**: Cross-dataset validation

### Methodology Advancement
- **Bayesian Optimization**: For higher efficiency
- **Neural Architecture Search**: For architecture optimization
- **Multi-Objective Optimization**: For multiple objectives
- **Ensemble Methods**: For better performance

### Application Development
- **Web Application**: User-friendly interface
- **Mobile Application**: TensorFlow Lite deployment
- **Educational Platform**: Batik learning system
- **Commercial Integration**: E-commerce applications

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Dataset**: Indonesian Batik Patterns Collection
- **Base Model**: MobileNetV2 (Google)
- **Framework**: TensorFlow/Keras
- **Hyperparameter Tuning**: Keras Tuner
- **Research Context**: Cultural Heritage Preservation

## ğŸ“š Additional Documentation

- **Detailed Hyperparameter Tuning**: `src/models/hyperparameterTuning/README.md`
- **Results Analysis**: `src/result/README_COMPARISON.md`
- **Baseline Evaluation**: `src/models/origin/baseline_evaluation.py`
- **Comparison Analysis**: `src/result/comparison_analysis.py`

---

**Research Title**: PERBANDINGAN HYPERPARAMETER TUNING BERBASIS TRANSFER LEARNING DALAM KLASIFIKASI CITRA BATIK

**Best Method**: Genetic Algorithm (98.33% accuracy, 16.96 minutes)

**Key Finding**: Genetic Algorithm provides the best balance between performance and computational efficiency for Indonesian batik classification using Transfer Learning. 