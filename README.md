# Batik Classification Project

A deep learning project for classifying Indonesian batik patterns using transfer learning with MobileNetV2.

## ğŸ“‹ Overview

This project implements a convolutional neural network (CNN) to classify 60 different Indonesian batik patterns. The model uses transfer learning with MobileNetV2 as the base architecture and includes comprehensive hyperparameter tuning using three different optimization methods.

## ğŸ¯ Project Structure

```
Batik-Final/
â”œâ”€â”€ dataset_split/                    # Dataset directory
â”‚   â”œâ”€â”€ train/                       # Training images (720 images)
â”‚   â”œâ”€â”€ val/                         # Validation images (180 images)
â”‚   â””â”€â”€ test/                        # Test images (180 images)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ origin/
â”‚       â”‚   â””â”€â”€ main.ipynb          # Baseline model implementation
â”‚       â””â”€â”€ hyperparameterTuning/
â”‚           â”œâ”€â”€ randomSearch/
â”‚           â”‚   â””â”€â”€ RandomSearch.ipynb
â”‚           â”œâ”€â”€ gridSearch/
â”‚           â”‚   â””â”€â”€ GridSearch.ipynb
â”‚           â”œâ”€â”€ geneticAlgorithm/
â”‚           â”‚   â””â”€â”€ GeneticAlgorithm.ipynb
â”‚           â””â”€â”€ README.md
â””â”€â”€ README.md                        # This file
```

## ğŸ“Š Dataset

### Dataset Information
- **Total Images**: 1,080
- **Classes**: 60 different batik patterns
- **Split**: 720 train / 180 validation / 180 test
- **Image Size**: 224x224 pixels
- **Format**: RGB images

### Batik Classes (60 patterns)
- Sekar Pijetan, Sekar Pacar, Gedhangan
- Sekar Keben, Sekar Jali, Mawur
- Sekar Duren, Sekar Dlima, Jayakirana
- Cinde Wilis, Sekar Blimbing, Sekar Ketongkeng
- And 48 more traditional batik patterns...

### Data Distribution
```
Class Distribution:
â”œâ”€â”€ Train: 12 images per class (720 total)
â”œâ”€â”€ Validation: 3 images per class (180 total)
â””â”€â”€ Test: 3 images per class (180 total)
```

## ğŸ¤– Model Architecture

### Base Model: MobileNetV2
- **Architecture**: Transfer learning with MobileNetV2
- **Pre-trained Weights**: ImageNet
- **Input Shape**: (224, 224, 3)
- **Feature Extraction**: Global Average Pooling

### Custom Classification Head
```python
Sequential([
    MobileNetV2(base),           # Feature extractor
    GlobalAveragePooling2D(),    # Flatten features
    Dense(128, activation='relu'), # Classification layer
    Dropout(0.2),               # Regularization
    Dense(60, activation='softmax') # Output layer
])
```

### Hyperparameters Optimized
1. **Learning Rate**: [1e-4, 3e-4, 1e-3, 3e-3, 1e-2]
2. **Optimizer**: [Adam, RMSprop, SGD]
3. **Dense Units**: [64, 128, 256, 384, 512]
4. **Dropout Rate**: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
5. **Add Conv Layer**: [True, False]
6. **Conv Filters**: [32, 64, 96, 128]

## ğŸš€ Usage

### Prerequisites
```bash
# Install required packages
pip install tensorflow keras numpy pandas matplotlib seaborn
pip install pillow tqdm scikit-learn kerastuner
```

### Quick Start

#### 1. Baseline Model
```bash
cd src/models/origin
jupyter notebook main.ipynb
```

#### 2. Hyperparameter Tuning

**Random Search** (Recommended for quick results):
```bash
cd src/models/hyperparameterTuning/randomSearch
jupyter notebook RandomSearch.ipynb
```

**Grid Search** (For small search spaces):
```bash
cd src/models/hyperparameterTuning/gridSearch
jupyter notebook GridSearch.ipynb
```

**Genetic Algorithm** (For large search spaces):
```bash
cd src/models/hyperparameterTuning/geneticAlgorithm
jupyter notebook GeneticAlgorithm.ipynb
```

### Training Process

1. **Data Loading**: Images are loaded and preprocessed
2. **Data Augmentation**: Applied to training data only
3. **Model Training**: Transfer learning with fine-tuning
4. **Evaluation**: Performance metrics on test set
5. **Model Saving**: Best model saved for deployment

### Expected Performance
- **Validation Accuracy**: ~98%
- **Test Accuracy**: ~97%
- **Training Time**: 15-30 minutes (depending on method)
- **Model Size**: ~14MB (efficient for deployment)

## ğŸ”§ Configuration

### Data Augmentation
```python
ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.15,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2]
)
```

### Training Parameters
- **Batch Size**: 4
- **Epochs**: 30 (with early stopping)
- **Learning Rate**: Adaptive (ReduceLROnPlateau)
- **Class Weights**: Balanced

## ğŸ“ˆ Results

### Model Performance
- **Best Validation Accuracy**: 98.89%
- **Best Test Accuracy**: 97.22%
- **Training Time**: ~25 minutes
- **Model Efficiency**: Suitable for mobile deployment

### Hyperparameter Tuning Results
| Method | Trials | Best Accuracy | Time |
|--------|--------|---------------|------|
| Random Search | 30 | 98.89% | 33 min |
| Grid Search | 100+ | 99.00% | 2+ hours |
| Genetic Algorithm | 10 gen | 97.50% | 45 min |

## ğŸ¯ Recommendations

### For Quick Results
- Use **Random Search** - Fast and efficient
- 20-30 trials usually sufficient
- Good for initial exploration

### For Optimal Results
- Use **Grid Search** - Guaranteed optimal solution
- Requires more time and computational resources
- Best for small, well-defined search spaces

### For Complex Optimization
- Use **Genetic Algorithm** - Efficient for large spaces
- Maintains diversity in search
- Can escape local optima

## ğŸ“ Notes

- **GPU Recommended**: Training is significantly faster with GPU
- **Memory Requirements**: ~8GB RAM recommended
- **Storage**: ~2GB for dataset and models
- **Compatibility**: Python 3.8+, TensorFlow 2.x

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Dataset: Indonesian Batik Patterns
- Base Model: MobileNetV2 (Google)
- Framework: TensorFlow/Keras
- Hyperparameter Tuning: Keras Tuner

---

**For detailed hyperparameter tuning documentation, see:** `src/models/hyperparameterTuning/README.md` 